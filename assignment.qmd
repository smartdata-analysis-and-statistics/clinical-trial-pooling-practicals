---
title: "Assignment: Pooled Analysis of Clinical Trial Data"
author: "Thomas Debray"
date: today
format: 
  pdf:
    number-sections: true
    toc: true
    toc-depth: 2
---

# Preparation for the Computer Practical

Before the session, please complete the following setup steps on your computer:

## Install R and RStudio
- Download and install R from: <https://cran.r-project.org/>
- Download and install RStudio (Desktop version) from: <https://posit.co/download/rstudio-desktop/>

## Install Required R Packages

Open RStudio and run the following code to install the necessary packages (skip any that are already installed):
```{r}
#| eval: false
install.packages(c("dplyr", "ggplot2", "lme4"))
```

## Download Supporting Scripts

Ensure the following R script is saved in your working directory (i.e., the folder you will use during the practical):

- `simulate_trials.R` You can download it from <https://github.com/smartdata-analysis-and-statistics/clinical-trial-pooling-practicals/> (or find it attached at the end of this document)


# Starting the Practical

Open RStudio

Load the required packages by running:

```{r}
#| eval: false
library(dplyr)
library(ggplot2)
library(lme4)
```

Source the simulation script to generate the dataset:

```{r}
#| eval: false
source("simulate_trials.R")  
```

This script will simulate patient-level data from multiple clinical trials. The resulting dataset will be used throughout the practical for modeling and analysis.

You're now ready to begin the exercises.

# Scenario 1

Use the `simulate_trials()` function to generate five clinical trials with a continuous outcome (`outcome`), a baseline measurement (`baseline`), and two treatment arms (active and control).

```{r}
#| eval: FALSE
df <- simulate_trials(
  baseline_means = rep(25, 5),
  trial_intercepts = rep(0, 5),
  cfb_active = rep(-9.6, 5),
  cfb_control = rep(-7.8, 5),
  n_per_arm = 50,
  seed = 2025
)
```

## Explore Baseline Distributions

Investigate the distribution of the `baseline` variable:

- Are there significant differences in baseline values between the two treatment arms within each trial?
- Does the baseline distribution vary between trials?

## Visual Diagnostics

Create a scatter plot showing individual `outcome` values as a function of `baseline`, separately for each trial.  An example is given by  [Torgersen et al. (2011)](https://www.tandfonline.com/doi/full/10.3109/00952990.2011.596980) (Figure 2). You can generate the plot as follows:

```{r}
#| eval: FALSE
ggplot(df, aes(x = baseline, y = outcome, color = trta)) +
  geom_point(alpha = 0.6) +
  facet_wrap(~trial) +
  labs(
    title = "Observed Outcomes by Baseline Severity and Trial",
    x = "Baseline Severity",
    y = "Outcome"
  ) +
  theme_minimal()
```

Use the plot to guide your expectations before fitting a model:

- Does the treatment effect appear to vary by trial?
- Does the treatment effect appear to vary with baseline severity?
- Based on the plot, would a naive pooled analysis (ignoring trial structure) seem appropriate?

## Estimation of treatment effect 

We now adopt a **naive pooling approach**, combining data from all trials and adjusting for baseline severity. The treatment effect (active vs. control) is estimated as follows:

```{r}
#| eval: FALSE
glm(outcome ~ trta + baseline, data = df)
```

After fitting the model, consider the following questions:

- What is the estimated treatment effect?
- Is the estimated effect consistent with the visual patterns in the plot?
- Do you think this model provides a valid summary of the treatment effect? Why or why not?

Now fit a stratified model that allows each trial to have its own intercept:

```{r}
#| eval: FALSE
glm(outcome ~ -1 + trial + trta + baseline, data = df)
```

This model accounts for differences in average outcome levels across trials, while still estimating a common treatment effect and adjusting for baseline severity.

- What is the estimated treatment effect (`trtaActive`)?
- Compare the trial-specific intercepts. Are they similar across trials, or is there meaningful variation?
- Do the intercept estimates align with what you observed in the scatter plot?
- Based on the model output, does the stratified approach seem more appropriate than the naive pooled analysis? Justify your answer.

# Scenario 2

Use the `simulate_trials()` function to generate five clinical trials with a continuous outcome, a baseline measurement, and two treatment arms. This time, we introduce systematic differences across trials:

```{r}
#| eval: FALSE
df <- simulate_trials(
  baseline_means = c(20, 25, 25, 30, 35),     
  trial_intercepts = c(5, 0, 0, -10, -15),
  cfb_active = rep(-9.6,5),
  cfb_control = rep(-7.8,5),
  n_per_arm = 50,
  seed = 2025
)
```

## Explore Baseline Distributions

Investigate the distribution of the `baseline` variable:

- Are there significant differences in baseline values between the two treatment arms within each trial?
- Does the baseline distribution vary between trials?


## Visual Diagnostics

Create a scatter plot showing individual `outcome` values as a function of `baseline`, separately for each trial.  Use the plot to guide your expectations before fitting a model:

- Does the treatment effect appear to vary by trial?
- Does the treatment effect appear to vary with baseline severity?
- Based on the plot, would a naive pooled analysis (ignoring trial structure) seem appropriate?

## Estimation of treatment effect 

- Estimate the treatment effect using naive pooling. 
- Estimate the treatment effect using a stratified model.
- Compare the results of the two models. How do they differ in terms of treatment effect estimates and trial intercepts?

Instead of estimating a separate fixed intercept for each trial, we can model trial-specific intercepts as random effects. This approach assumes that the intercepts for each trial are not fixed but drawn from a common normal distribution.

```{r}
#| eval: FALSE
lmer(outcome ~  trta + baseline + (1|trial), data = df)
```

- Extract the estimated effects using the function `coef()`
- What is the estimated treatment effect?
- What are the estimated trial-specific intercepts?
- How do these intercepts compare to those from the stratified model? **Tip**: You may compare the standard deviation of the trial intercepts across models to assess how much variability is captured or shrunk in the random-effects approach.


# Scenario 3

Use the `simulate_trials()` function to generate five clinical trials with a continuous outcome, a baseline measurement, and two treatment arms. This time, we introduce heterogeneity in the treatment effect.

```{r}
#| eval: FALSE
df <- simulate_trials(
  baseline_means = c(20, 25, 25, 30, 35),     
  trial_intercepts = c(5, 0, 0, -10, -15),
  cfb_active = rep(-9.6,5),
  cfb_control = rep(-7.8,5),
  n_per_arm = 50,
  beta_W = rep(-0.15,5),  
  beta_A = 0.35,
  seed = 2025
)
```

## Visual Diagnostics

Create a scatter plot showing individual `outcome` values as a function of `baseline`, separately for each trial.  Use the plot to guide your expectations before fitting a model:

- Does the treatment effect appear to vary by trial?
- Does the treatment effect appear to vary with baseline severity?
- Based on the plot, would a naive pooled analysis (ignoring trial structure) seem appropriate?


## Estimation of treatment effect 

In this scenario, we investigate the presence of heterogeneous treatment effects using a linear mixed model with random slopes for treatment and fixed intercepts by trial:

```{r}
#| eval: false
lmer(outcome ~ -1 + trial +  baseline:trial + trt + 
       (0 + trt | trial), data = df)
```

In this formula:

- `-1 + trial`: Fixed intercept per trial
- `baseline:trial` Fixed effect for baseline severity per trial
- `trt`: Pooled treatment effect
- `(0 + trt | trial)`: Random treatment effect per trial

We can extract key parameters as follows:

```{r}
#| eval: false
# Pooled treatment effect
fixef(fit)["trt"]

# Standard error of the treatment effect
sqrt(vcov(fit)["trt", "trt"])

# Between-study variance of the treatment effect
VarCorr(fit)$trial["trt", "trt"]
```

Use the estimates above to derive an approximate 95\% confidence interval and 95\% prediction interval for the treatment effect.

Based on the model output:

- What is the estimated between-trial variance of the treatment effect (τ²)?
- How does the 95\% prediction interval compare to the confidence interval of the pooled treatment effect?
- Do the trial-specific treatment effects (from the plot or model output) vary meaningfully across trials?
- What might this imply about the assumption of a common treatment effect?

Optional: Plot the trial-specific treatment effects and overlay the pooled estimate with its prediction interval.

## Modeling treatment-effect modification

We suspect that the heterogeneity in treatment effect may be explained by differences in baseline severity across patients. To explore this, we include a common interaction effect between treatment and baseline in the model:

```{r}
#| eval: FALSE
lmer(outcome ~ -1 + trial + baseline:trial + trt + 
       trt:baseline + (0 + trt | trial) , data = df)
```

- Which parameter in the model quantifies the pooled interaction effect? 
- What does the *sign* and *magnitude* of the interaction coefficient (`trt:baseline`) tell you?
- How should we interpret this interaction term in the context of treatment-effect modification?
- Is the model appropriate for separating within- and across-study interaction effects?
- How can we evaluate whether residual heterogeneity in treatment effect remains, despite modeling the interaction?

We now fit a regression model that explicitly separates within- from across-trial interaction:

```{r}
#| eval: FALSE
lmer(outcome ~ -1 + trial + baseline:trial + trt:trial + 
       trt:baseline + (0 + trt:baseline | trial) , data = df)
```

In this formula:

- `-1 + trial`: Fixed intercept per trial
- `baseline:trial`: Fixed effect for baseline severity per trial
- `trt:trial` : Fixed treatment effect per trial
- `baseline:trt`: Pooled within-study interaction between baseline and treatment
- `(0 + trt:baseline | trial)`: Trial-specific deviations from the pooled interaction

**Questions for Discussion**

- How does this model help distinguish within- from across-trial interactions?
- What does the variance component for the random slope `(0 + trt:baseline | trial)` represent?
- How can we use this model to examine inconsistency or effect modification across trials?

Finally, we consider an extended model that also adjusts for trial-level confounding using study-level means:

```{r}
#| eval: FALSE
lmer(
  outcome ~ -1 + trial + trt + trial:baseline + trt:cbase +xk_mean:trt+
    (0 + trt | trial) + (0 + trt:cbase | trial),
  data = df
)
```

In this formula:

- `xk_mean:trt`: Common effect for study-level confounding
- `trt:cbase`: Pooled within-study interaction between baseline and treatment
- ` (0 + trt | trial)`: Random treatment effects
- `(0 + trt:cbase | trial)`: Random interaction effects


**Questions for Discussion**

- What role does `xk_mean:trt` play in this model? How does it address ecological bias?
- Under what conditions might the inclusion of study-level means improve model validity?
- How do you interpret the presence of both fixed and random interaction terms in this model?
- Would you expect this model to reduce between-trial heterogeneity in treatment effects? Why or why not?


# Concluding remarks

The solutions to the practical are available from <https://smartdata-analysis-and-statistics.github.io/clinical-trial-pooling-practicals/>

# APPENDIX

The contents of `simulate_trials.R` are as follows:
```{r}
#| eval: TRUE
#| echo: FALSE
# Show contents of the script
cat(readLines("R/simulate_trials.R"), sep = "\n")
```
